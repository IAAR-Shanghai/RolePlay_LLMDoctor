import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import init_empty_weights, infer_auto_device_map, dispatch_model
import os 
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
def load_llm(MODEL_NAME):
    model_path = f"XXXXX" # æ›¿æ¢ä¸ºå®é™…æ¨¡å‹è·¯å¾„
    is_llama = "llama" in MODEL_NAME.lower()
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=not is_llama, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    def get_model_size(name):
        name = name.lower()
        if "72b" in name: return 72
        if "65b" in name: return 65
        if "32b" in name: return 32
        if "14b" in name: return 14
        return 7  # é»˜è®¤

    model_size = get_model_size(MODEL_NAME)

    visible_gpus = os.environ.get("CUDA_VISIBLE_DEVICES", "")
    if visible_gpus:
        gpu_indices = list(range(len(visible_gpus.split(","))))
    else:
        gpu_indices = list(range(torch.cuda.device_count()))
    print(f"ğŸ–¥ï¸ å¯ç”¨ GPU: {gpu_indices}")

    print(f"ğŸ“¦ ä½¿ç”¨{len(gpu_indices)}å¼ æ˜¾å¡åŠ è½½ {MODEL_NAME}ï¼ˆ{model_size}Bï¼‰")
    model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            torch_dtype=dtype,
            trust_remote_code=True
    )

    model.eval()
    return model, tokenizer
